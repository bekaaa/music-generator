{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from music21 import note, chord, instrument, converter\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true for debugging\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes are loaded properly using the saved pickle\n",
      "Input shape =  (620, 100, 1) \n",
      "Output shape =  (620, 32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "=================================================================\n",
      "Total params: 5,390,624\n",
      "Trainable params: 5,390,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "620/620 [==============================] - 147s 237ms/step - loss: 3.1223\n",
      "Epoch 2/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.9273\n",
      "Epoch 3/200\n",
      "620/620 [==============================] - 117s 189ms/step - loss: 2.8666\n",
      "Epoch 4/200\n",
      "620/620 [==============================] - 116s 187ms/step - loss: 2.8963\n",
      "Epoch 5/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.8748\n",
      "Epoch 6/200\n",
      "620/620 [==============================] - 115s 186ms/step - loss: 2.8786\n",
      "Epoch 7/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 3.2411\n",
      "Epoch 8/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.9539\n",
      "Epoch 9/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.9503\n",
      "Epoch 10/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.8937\n",
      "Epoch 11/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8985\n",
      "Epoch 12/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.8742\n",
      "Epoch 13/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.8651\n",
      "Epoch 14/200\n",
      "620/620 [==============================] - 135s 217ms/step - loss: 2.8711\n",
      "Epoch 15/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.8637\n",
      "Epoch 16/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8626\n",
      "Epoch 17/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8540\n",
      "Epoch 18/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.8568\n",
      "Epoch 19/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8697\n",
      "Epoch 20/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8601\n",
      "Epoch 21/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.8828\n",
      "Epoch 22/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.8477\n",
      "Epoch 23/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8550\n",
      "Epoch 24/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8558\n",
      "Epoch 25/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8429\n",
      "Epoch 26/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8435\n",
      "Epoch 27/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8499\n",
      "Epoch 28/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8441\n",
      "Epoch 29/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8405\n",
      "Epoch 30/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8509\n",
      "Epoch 31/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8612\n",
      "Epoch 32/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8588\n",
      "Epoch 33/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.8595\n",
      "Epoch 34/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8496\n",
      "Epoch 35/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8524\n",
      "Epoch 36/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.8368\n",
      "Epoch 37/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8550\n",
      "Epoch 38/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8553\n",
      "Epoch 39/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.8450\n",
      "Epoch 40/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8433\n",
      "Epoch 41/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8451\n",
      "Epoch 42/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8369\n",
      "Epoch 43/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8305\n",
      "Epoch 44/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.8227\n",
      "Epoch 45/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8221\n",
      "Epoch 46/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.8356\n",
      "Epoch 47/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.8330\n",
      "Epoch 48/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.8118\n",
      "Epoch 49/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8345\n",
      "Epoch 50/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8319\n",
      "Epoch 51/200\n",
      "620/620 [==============================] - 124s 199ms/step - loss: 2.8096\n",
      "Epoch 52/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8121\n",
      "Epoch 53/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8087\n",
      "Epoch 54/200\n",
      "620/620 [==============================] - 123s 198ms/step - loss: 2.7944\n",
      "Epoch 55/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7899\n",
      "Epoch 56/200\n",
      "620/620 [==============================] - 128s 206ms/step - loss: 2.7850\n",
      "Epoch 57/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7809\n",
      "Epoch 58/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7769\n",
      "Epoch 59/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7790\n",
      "Epoch 60/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7695\n",
      "Epoch 61/200\n",
      "620/620 [==============================] - 129s 209ms/step - loss: 2.7773\n",
      "Epoch 62/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7722\n",
      "Epoch 63/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7659\n",
      "Epoch 64/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7763\n",
      "Epoch 65/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7693\n",
      "Epoch 66/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7582\n",
      "Epoch 67/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7717\n",
      "Epoch 68/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7584\n",
      "Epoch 69/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7628\n",
      "Epoch 70/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7700\n",
      "Epoch 71/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7571\n",
      "Epoch 72/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7580\n",
      "Epoch 73/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7454\n",
      "Epoch 74/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200\n",
      "620/620 [==============================] - 129s 209ms/step - loss: 2.7580\n",
      "Epoch 76/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7519\n",
      "Epoch 77/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7533\n",
      "Epoch 78/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7615\n",
      "Epoch 79/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7455\n",
      "Epoch 80/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7482\n",
      "Epoch 81/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7485\n",
      "Epoch 82/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7780\n",
      "Epoch 83/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7755\n",
      "Epoch 84/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7679\n",
      "Epoch 85/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7415\n",
      "Epoch 86/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7289\n",
      "Epoch 87/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7495\n",
      "Epoch 88/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7452\n",
      "Epoch 89/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7690\n",
      "Epoch 90/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7623\n",
      "Epoch 91/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7525\n",
      "Epoch 92/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.7597\n",
      "Epoch 93/200\n",
      "160/620 [======>.......................] - ETA: 1:39 - loss: 2.6875"
     ]
    }
   ],
   "source": [
    "# Main function to call all subfunctions in the notebook.\n",
    "def train_network():\n",
    "    # get data and convert it to notes\n",
    "    notes, n_vocab = get_notes(quick=True)\n",
    "    # prepare data\n",
    "    mapped_notes, NetworkInput, NetworkOutut = prepare_data(notes, n_vocab)\n",
    "    # get model\n",
    "    model = get_model(NetworkInput.shape[1:], n_vocab)\n",
    "    # train\n",
    "    train(model, NetworkInput, NetworkOutut)\n",
    "#--------------------------------------------------------------------\n",
    "# uncomment after runing all cells.\n",
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(quick=False):\n",
    "    ''' Read input midi files and convert them to notes\n",
    "        Also quick refers to using the saved notes.pkl to retrieve notes instead of reading midi files.\n",
    "    '''\n",
    "    input_folder = './data/input_midi/'\n",
    "    output_folder = './data/'\n",
    "    notes = []\n",
    "    \n",
    "    if quick :\n",
    "        try :\n",
    "            with open(output_folder+'notes.pkl','rb') as f :\n",
    "                notes = pickle.load(f)\n",
    "            print('Notes are loaded properly using the saved pickle')\n",
    "            return notes, len(set(notes))\n",
    "        except :\n",
    "            print('It\\'s not possible to do it quick,\\n reading midi files....')\n",
    "            return get_notes()\n",
    "    \n",
    "    for file in glob.glob(input_folder+'*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse  = []\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "        \n",
    "        if parts :\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else :\n",
    "            notes_to_parse = parts.flat_notes\n",
    "        \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(e) for e in c.pitches))\n",
    "            else : # it shouldn't reach here.\n",
    "                pass\n",
    "        with open(output_folder+'notes.pkl', 'wb') as f :\n",
    "            pickle.dump(notes, f)\n",
    "        print('data loaded properly and saved to disk as notes.pkl.')\n",
    "        return notes, len(set(notes))\n",
    "#--------------------------------------------------------\n",
    "if debug : notes, n_vocab = get_notes(quick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(notes, n_vocab):\n",
    "    ''' create input sequences and output notes '''\n",
    "    sequence_length = 100\n",
    "    NetworkInput = []\n",
    "    NetworkOutput = []\n",
    "    # create a mapping to the notes\n",
    "    mapper = LabelEncoder()\n",
    "    mapped_notes = mapper.fit_transform(notes)\n",
    "    #note_to_int = dict((note,i) for i,note in enumerate(notes))\n",
    "    \n",
    "    for i in range(len(notes)-sequence_length):\n",
    "        in_seq = mapped_notes[i : i+sequence_length]\n",
    "        out_note = mapped_notes[i+sequence_length]\n",
    "        NetworkInput.append(in_seq)\n",
    "        NetworkOutput.append(out_note)\n",
    "        #NetworkInput.append( [ note_to_int[note] for note in in_seq ] )\n",
    "        #NetworkOutput.append(note_to_int[out_note])\n",
    "    \n",
    "    n_patterns = len(NetworkOutput)\n",
    "    \n",
    "    NetworkInput = np.reshape(NetworkInput, (n_patterns, sequence_length, 1))\n",
    "    NetworkInput = NetworkInput / float(n_vocab)\n",
    "    \n",
    "    NetworkOutput = np.reshape(NetworkOutput, (-1,1))\n",
    "    hotencoder = OneHotEncoder(sparse=False)\n",
    "    _ = hotencoder.fit(mapped_notes.reshape(-1,1))\n",
    "    NetworkOutput = hotencoder.transform(NetworkOutput)\n",
    "    \n",
    "    #NetworkOutput = np_utils.to_categorical(NetworkOutput)\n",
    "    #NetworkOutput = NetworkOutput.reshape(-1,NetworkOutput.shape[1],1)\n",
    "    print('Input shape = ',NetworkInput.shape, '\\nOutput shape = ', NetworkOutput.shape)\n",
    "    return mapped_notes, NetworkInput, NetworkOutput\n",
    "#---------------------------------------\n",
    "if debug : mapped_notes, NetworkInput, NetworkOutput = prepare_data(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, n_vocab):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=True)(X_input)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=True)(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=False)(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = Dense(256, activation='tanh')(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = Dense(n_vocab, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(X_input, X)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "#----------------------------------------------\n",
    "if debug : model = get_model(NetworkInput.shape[1:], n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NetworkInput, NetworkOutput):\n",
    "    filepath = './checkpoints/0/ckpt-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks=[checkpoint]\n",
    "    \n",
    "    model.fit(NetworkInput, NetworkOutput, epochs=200, batch_size=16, callbacks=callbacks)\n",
    "    return model\n",
    "#-----------------------------------\n",
    "if debug : trained_model = train(model, NetworkInput, NetworkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
