{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from music21 import note, chord, instrument, converter\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true for debugging\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes are loaded properly using the saved pickle\n",
      "Input shape =  (620, 100, 1) \n",
      "Output shape =  (620, 32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "=================================================================\n",
      "Total params: 5,390,624\n",
      "Trainable params: 5,390,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "620/620 [==============================] - 147s 237ms/step - loss: 3.1223\n",
      "Epoch 2/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.9273\n",
      "Epoch 3/200\n",
      "620/620 [==============================] - 117s 189ms/step - loss: 2.8666\n",
      "Epoch 4/200\n",
      "620/620 [==============================] - 116s 187ms/step - loss: 2.8963\n",
      "Epoch 5/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.8748\n",
      "Epoch 6/200\n",
      "620/620 [==============================] - 115s 186ms/step - loss: 2.8786\n",
      "Epoch 7/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 3.2411\n",
      "Epoch 8/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.9539\n",
      "Epoch 9/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.9503\n",
      "Epoch 10/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.8937\n",
      "Epoch 11/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8985\n",
      "Epoch 12/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.8742\n",
      "Epoch 13/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.8651\n",
      "Epoch 14/200\n",
      "620/620 [==============================] - 135s 217ms/step - loss: 2.8711\n",
      "Epoch 15/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.8637\n",
      "Epoch 16/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8626\n",
      "Epoch 17/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8540\n",
      "Epoch 18/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.8568\n",
      "Epoch 19/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8697\n",
      "Epoch 20/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8601\n",
      "Epoch 21/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.8828\n",
      "Epoch 22/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.8477\n",
      "Epoch 23/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8550\n",
      "Epoch 24/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8558\n",
      "Epoch 25/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.8429\n",
      "Epoch 26/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8435\n",
      "Epoch 27/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8499\n",
      "Epoch 28/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8441\n",
      "Epoch 29/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8405\n",
      "Epoch 30/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8509\n",
      "Epoch 31/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8612\n",
      "Epoch 32/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.8588\n",
      "Epoch 33/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.8595\n",
      "Epoch 34/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8496\n",
      "Epoch 35/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8524\n",
      "Epoch 36/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.8368\n",
      "Epoch 37/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.8550\n",
      "Epoch 38/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8553\n",
      "Epoch 39/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.8450\n",
      "Epoch 40/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8433\n",
      "Epoch 41/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8451\n",
      "Epoch 42/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.8369\n",
      "Epoch 43/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8305\n",
      "Epoch 44/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.8227\n",
      "Epoch 45/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.8221\n",
      "Epoch 46/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.8356\n",
      "Epoch 47/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.8330\n",
      "Epoch 48/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.8118\n",
      "Epoch 49/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8345\n",
      "Epoch 50/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8319\n",
      "Epoch 51/200\n",
      "620/620 [==============================] - 124s 199ms/step - loss: 2.8096\n",
      "Epoch 52/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.8121\n",
      "Epoch 53/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.8087\n",
      "Epoch 54/200\n",
      "620/620 [==============================] - 123s 198ms/step - loss: 2.7944\n",
      "Epoch 55/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7899\n",
      "Epoch 56/200\n",
      "620/620 [==============================] - 128s 206ms/step - loss: 2.7850\n",
      "Epoch 57/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7809\n",
      "Epoch 58/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7769\n",
      "Epoch 59/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7790\n",
      "Epoch 60/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7695\n",
      "Epoch 61/200\n",
      "620/620 [==============================] - 129s 209ms/step - loss: 2.7773\n",
      "Epoch 62/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7722\n",
      "Epoch 63/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7659\n",
      "Epoch 64/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7763\n",
      "Epoch 65/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7693\n",
      "Epoch 66/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7582\n",
      "Epoch 67/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7717\n",
      "Epoch 68/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7584\n",
      "Epoch 69/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7628\n",
      "Epoch 70/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7700\n",
      "Epoch 71/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7571\n",
      "Epoch 72/200\n",
      "620/620 [==============================] - 129s 208ms/step - loss: 2.7580\n",
      "Epoch 73/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7454\n",
      "Epoch 74/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200\n",
      "620/620 [==============================] - 129s 209ms/step - loss: 2.7580\n",
      "Epoch 76/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7519\n",
      "Epoch 77/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7533\n",
      "Epoch 78/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7615\n",
      "Epoch 79/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7455\n",
      "Epoch 80/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7482\n",
      "Epoch 81/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7485\n",
      "Epoch 82/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7780\n",
      "Epoch 83/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7755\n",
      "Epoch 84/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7679\n",
      "Epoch 85/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7415\n",
      "Epoch 86/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7289\n",
      "Epoch 87/200\n",
      "620/620 [==============================] - 130s 210ms/step - loss: 2.7495\n",
      "Epoch 88/200\n",
      "620/620 [==============================] - 128s 207ms/step - loss: 2.7452\n",
      "Epoch 89/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7690\n",
      "Epoch 90/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7623\n",
      "Epoch 91/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7525\n",
      "Epoch 92/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.7597\n",
      "Epoch 93/200\n",
      "620/620 [==============================] - 135s 218ms/step - loss: 2.7605\n",
      "Epoch 94/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7572\n",
      "Epoch 95/200\n",
      "620/620 [==============================] - 129s 209ms/step - loss: 2.7588\n",
      "Epoch 96/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7561\n",
      "Epoch 97/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7561\n",
      "Epoch 98/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7515\n",
      "Epoch 99/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7521\n",
      "Epoch 100/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.7475\n",
      "Epoch 101/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7367\n",
      "Epoch 102/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7334\n",
      "Epoch 103/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.7481\n",
      "Epoch 104/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.7391\n",
      "Epoch 105/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.7314\n",
      "Epoch 106/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7355\n",
      "Epoch 107/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.7437\n",
      "Epoch 108/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7338\n",
      "Epoch 109/200\n",
      "620/620 [==============================] - 126s 202ms/step - loss: 2.7320\n",
      "Epoch 110/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7398\n",
      "Epoch 111/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7271\n",
      "Epoch 112/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.7412\n",
      "Epoch 113/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7256\n",
      "Epoch 114/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7414\n",
      "Epoch 115/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.7622\n",
      "Epoch 116/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7287\n",
      "Epoch 117/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7314\n",
      "Epoch 118/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7273\n",
      "Epoch 119/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.7204\n",
      "Epoch 120/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.7492\n",
      "Epoch 121/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.7663\n",
      "Epoch 122/200\n",
      "620/620 [==============================] - 128s 206ms/step - loss: 2.7419\n",
      "Epoch 123/200\n",
      "620/620 [==============================] - 127s 204ms/step - loss: 2.7369\n",
      "Epoch 124/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.7256\n",
      "Epoch 125/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.7319\n",
      "Epoch 126/200\n",
      "620/620 [==============================] - 128s 206ms/step - loss: 2.7300\n",
      "Epoch 127/200\n",
      "620/620 [==============================] - 127s 205ms/step - loss: 2.7164\n",
      "Epoch 128/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7535\n",
      "Epoch 129/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7241\n",
      "Epoch 130/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7229\n",
      "Epoch 131/200\n",
      "620/620 [==============================] - 125s 201ms/step - loss: 2.7548\n",
      "Epoch 132/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7458\n",
      "Epoch 133/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7188\n",
      "Epoch 134/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7418\n",
      "Epoch 135/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7294\n",
      "Epoch 136/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7790\n",
      "Epoch 137/200\n",
      "620/620 [==============================] - 126s 204ms/step - loss: 2.7674\n",
      "Epoch 138/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7450\n",
      "Epoch 139/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7375\n",
      "Epoch 140/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7669\n",
      "Epoch 141/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7529\n",
      "Epoch 142/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7445\n",
      "Epoch 143/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.7326\n",
      "Epoch 144/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.7390\n",
      "Epoch 145/200\n",
      "620/620 [==============================] - 131s 212ms/step - loss: 2.7574\n",
      "Epoch 146/200\n",
      "620/620 [==============================] - 131s 211ms/step - loss: 2.7381\n",
      "Epoch 147/200\n",
      "620/620 [==============================] - 132s 212ms/step - loss: 2.7340\n",
      "Epoch 148/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7344\n",
      "Epoch 149/200\n",
      "620/620 [==============================] - 132s 213ms/step - loss: 2.7375\n",
      "Epoch 150/200\n",
      "620/620 [==============================] - 136s 219ms/step - loss: 2.7376\n",
      "Epoch 151/200\n",
      "620/620 [==============================] - 140s 225ms/step - loss: 2.7352\n",
      "Epoch 152/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.7469\n",
      "Epoch 153/200\n",
      "620/620 [==============================] - 124s 199ms/step - loss: 2.7385\n",
      "Epoch 154/200\n",
      "620/620 [==============================] - 130s 209ms/step - loss: 2.7476\n",
      "Epoch 155/200\n",
      "620/620 [==============================] - 123s 199ms/step - loss: 2.7424\n",
      "Epoch 156/200\n",
      "620/620 [==============================] - 124s 201ms/step - loss: 2.7421\n",
      "Epoch 157/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7239\n",
      "Epoch 158/200\n",
      "620/620 [==============================] - 123s 199ms/step - loss: 2.7344\n",
      "Epoch 159/200\n",
      "620/620 [==============================] - 125s 202ms/step - loss: 2.7271\n",
      "Epoch 160/200\n",
      "620/620 [==============================] - 126s 203ms/step - loss: 2.7280\n",
      "Epoch 161/200\n",
      "620/620 [==============================] - 124s 200ms/step - loss: 2.7425\n",
      "Epoch 162/200\n",
      "620/620 [==============================] - 123s 199ms/step - loss: 2.7459\n",
      "Epoch 163/200\n",
      "620/620 [==============================] - 139s 224ms/step - loss: 2.7396\n",
      "Epoch 164/200\n",
      "620/620 [==============================] - 134s 217ms/step - loss: 2.7436\n",
      "Epoch 165/200\n",
      "620/620 [==============================] - 135s 218ms/step - loss: 2.7376\n",
      "Epoch 166/200\n",
      "620/620 [==============================] - 135s 217ms/step - loss: 2.7343\n",
      "Epoch 167/200\n",
      "620/620 [==============================] - 134s 217ms/step - loss: 2.7465\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 133s 214ms/step - loss: 2.7255\n",
      "Epoch 169/200\n",
      "620/620 [==============================] - 134s 216ms/step - loss: 2.7248\n",
      "Epoch 170/200\n",
      "620/620 [==============================] - 134s 216ms/step - loss: 2.7407\n",
      "Epoch 171/200\n",
      "620/620 [==============================] - 136s 219ms/step - loss: 2.7297\n",
      "Epoch 172/200\n",
      "620/620 [==============================] - 137s 222ms/step - loss: 2.7446\n",
      "Epoch 173/200\n",
      "620/620 [==============================] - 134s 216ms/step - loss: 2.7267\n",
      "Epoch 174/200\n",
      "620/620 [==============================] - 135s 217ms/step - loss: 2.7363\n",
      "Epoch 175/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.7278\n",
      "Epoch 176/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.7408\n",
      "Epoch 177/200\n",
      "620/620 [==============================] - 134s 217ms/step - loss: 2.7353\n",
      "Epoch 178/200\n",
      "620/620 [==============================] - 138s 223ms/step - loss: 2.7364\n",
      "Epoch 179/200\n",
      "620/620 [==============================] - 135s 218ms/step - loss: 2.7278\n",
      "Epoch 180/200\n",
      "620/620 [==============================] - 133s 214ms/step - loss: 2.7381\n",
      "Epoch 181/200\n",
      "620/620 [==============================] - 133s 215ms/step - loss: 2.7232\n",
      "Epoch 182/200\n",
      "620/620 [==============================] - 134s 216ms/step - loss: 2.7189\n",
      "Epoch 183/200\n",
      "620/620 [==============================] - 142s 229ms/step - loss: 2.7219\n",
      "Epoch 184/200\n",
      "620/620 [==============================] - 118s 190ms/step - loss: 2.7115\n",
      "Epoch 185/200\n",
      "620/620 [==============================] - 115s 186ms/step - loss: 2.7297\n",
      "Epoch 186/200\n",
      "620/620 [==============================] - 115s 186ms/step - loss: 2.7296\n",
      "Epoch 187/200\n",
      "620/620 [==============================] - 115s 186ms/step - loss: 2.7270\n",
      "Epoch 188/200\n",
      "620/620 [==============================] - 115s 185ms/step - loss: 2.7099\n",
      "Epoch 189/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.7183\n",
      "Epoch 190/200\n",
      "620/620 [==============================] - 113s 182ms/step - loss: 2.7259\n",
      "Epoch 191/200\n",
      "620/620 [==============================] - 113s 183ms/step - loss: 2.7306\n",
      "Epoch 192/200\n",
      "620/620 [==============================] - 113s 182ms/step - loss: 2.7214\n",
      "Epoch 193/200\n",
      "620/620 [==============================] - 114s 183ms/step - loss: 2.7245\n",
      "Epoch 194/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.7170\n",
      "Epoch 195/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.7221\n",
      "Epoch 196/200\n",
      "620/620 [==============================] - 114s 183ms/step - loss: 2.7163\n",
      "Epoch 197/200\n",
      "620/620 [==============================] - 114s 184ms/step - loss: 2.7375\n",
      "Epoch 198/200\n",
      "620/620 [==============================] - 113s 183ms/step - loss: 2.7244\n",
      "Epoch 199/200\n",
      "620/620 [==============================] - 113s 182ms/step - loss: 2.7687\n",
      "Epoch 200/200\n",
      "620/620 [==============================] - 113s 183ms/step - loss: 2.7171\n"
     ]
    }
   ],
   "source": [
    "# Main function to call all subfunctions in the notebook.\n",
    "def train_network():\n",
    "    # get data and convert it to notes\n",
    "    notes, n_vocab = get_notes(quick=True)\n",
    "    # prepare data\n",
    "    mapped_notes, NetworkInput, NetworkOutut = prepare_data(notes, n_vocab)\n",
    "    # get model\n",
    "    model = get_model(NetworkInput.shape[1:], n_vocab)\n",
    "    # train\n",
    "    train(model, NetworkInput, NetworkOutut)\n",
    "#--------------------------------------------------------------------\n",
    "# uncomment after runing all cells.\n",
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(quick=False):\n",
    "    ''' Read input midi files and convert them to notes\n",
    "        Also quick refers to using the saved notes.pkl to retrieve notes instead of reading midi files.\n",
    "    '''\n",
    "    input_folder = './data/input_midi/'\n",
    "    output_folder = './data/'\n",
    "    notes = []\n",
    "    \n",
    "    if quick :\n",
    "        try :\n",
    "            with open(output_folder+'notes.pkl','rb') as f :\n",
    "                notes = pickle.load(f)\n",
    "            print('Notes are loaded properly using the saved pickle')\n",
    "            return notes, len(set(notes))\n",
    "        except :\n",
    "            print('It\\'s not possible to do it quick,\\n reading midi files....')\n",
    "            return get_notes()\n",
    "    \n",
    "    for file in glob.glob(input_folder+'*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse  = []\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "        \n",
    "        if parts :\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else :\n",
    "            notes_to_parse = parts.flat_notes\n",
    "        \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(e) for e in c.pitches))\n",
    "            else : # it shouldn't reach here.\n",
    "                pass\n",
    "        with open(output_folder+'notes.pkl', 'wb') as f :\n",
    "            pickle.dump(notes, f)\n",
    "        print('data loaded properly and saved to disk as notes.pkl.')\n",
    "        return notes, len(set(notes))\n",
    "#--------------------------------------------------------\n",
    "if debug : notes, n_vocab = get_notes(quick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(notes, n_vocab):\n",
    "    ''' create input sequences and output notes '''\n",
    "    sequence_length = 100\n",
    "    NetworkInput = []\n",
    "    NetworkOutput = []\n",
    "    # create a mapping to the notes\n",
    "    mapper = LabelEncoder()\n",
    "    mapped_notes = mapper.fit_transform(notes)\n",
    "\n",
    "    for i in range(len(notes)-sequence_length):\n",
    "        in_seq = mapped_notes[i : i+sequence_length]\n",
    "        out_note = mapped_notes[i+sequence_length]\n",
    "        NetworkInput.append(in_seq)\n",
    "        NetworkOutput.append(out_note)\n",
    "    \n",
    "    n_patterns = len(NetworkOutput)\n",
    "    \n",
    "    NetworkInput = np.reshape(NetworkInput, (n_patterns, sequence_length, 1))\n",
    "    NetworkInput = NetworkInput / float(n_vocab)\n",
    "    \n",
    "    NetworkOutput = np.reshape(NetworkOutput, (-1,1))\n",
    "    hotencoder = OneHotEncoder(sparse=False)\n",
    "    _ = hotencoder.fit(mapped_notes.reshape(-1,1))\n",
    "    NetworkOutput = hotencoder.transform(NetworkOutput)\n",
    "    \n",
    "    # save the mapper and hotencoder to disk for prediction.\n",
    "    #with open('./data/mapper.pkl','wb') as f:\n",
    "    #    pickle.dump(mapper, f)\n",
    "    #with open('./data/hotencoder.pkl','wb') as f:\n",
    "    #    pickle.dump(hotencoder, f)\n",
    "    \n",
    "    print('Input shape = ',NetworkInput.shape, '\\nOutput shape = ', NetworkOutput.shape)\n",
    "    return mapped_notes, NetworkInput, NetworkOutput\n",
    "#---------------------------------------\n",
    "if debug : mapped_notes, NetworkInput, NetworkOutput = prepare_data(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, n_vocab):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=True)(X_input)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=True)(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = LSTM(512, activation='tanh', return_sequences=False)(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = Dense(256, activation='tanh')(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = Dense(n_vocab, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(X_input, X)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "#----------------------------------------------\n",
    "if debug : model = get_model(NetworkInput.shape[1:], n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NetworkInput, NetworkOutput):\n",
    "    filepath = './checkpoints/0/ckpt-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks=[checkpoint]\n",
    "    \n",
    "    model.fit(NetworkInput, NetworkOutput, epochs=1000, batch_size=16, callbacks=callbacks)\n",
    "    return model\n",
    "#-----------------------------------\n",
    "if debug : trained_model = train(model, NetworkInput, NetworkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
