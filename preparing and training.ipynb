{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from music21 import note, chord, instrument, converter\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true for debugging\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to call all subfunctions in the notebook.\n",
    "def train_network():\n",
    "    # get data and convert it to notes\n",
    "    notes, n_vocab = get_notes(quick=False)\n",
    "    # prepare data\n",
    "    mapped_notes, NetworkInput, NetworkOutut = prepare_data(notes, n_vocab)\n",
    "    # get model\n",
    "    model = get_model(NetworkInput.shape[1:], n_vocab)\n",
    "    # train\n",
    "    train(model, NetworkInput, NetworkOutut)\n",
    "#--------------------------------------------------------------------\n",
    "# uncomment after runing all cells.\n",
    "#train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(quick=False):\n",
    "    ''' Read input midi files and convert them to notes\n",
    "        Also quick refers to using the saved notes.pkl to retrieve notes instead of reading midi files.\n",
    "    '''\n",
    "    input_folder = './music/Mozart/'\n",
    "    output_notes_file = './data/third_piece_MozartNotes.pkl'\n",
    "    notes = []\n",
    "    \n",
    "    if quick :\n",
    "        try :\n",
    "            with open(output_notes_file,'rb') as f :\n",
    "                notes = pickle.load(f)\n",
    "            print('Notes are loaded properly using the saved pickle')\n",
    "            return notes, len(set(notes))\n",
    "        except :\n",
    "            print('It\\'s not possible to do it quick,\\n reading midi files....')\n",
    "            return get_notes()\n",
    "    \n",
    "    for index,file in enumerate(glob.glob(input_folder+'*.mid')):\n",
    "        if index % 10 == 0 : print(index,end='')\n",
    "        print('.',end='')\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse  = []\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "        \n",
    "        if parts :\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else :\n",
    "            notes_to_parse = parts.flat_notes\n",
    "        \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(e) for e in element.pitches))\n",
    "            else : # it shouldn't reach here.\n",
    "                pass\n",
    "        \n",
    "    with open(output_notes_file, 'wb') as f :\n",
    "        pickle.dump(notes, f)\n",
    "    print('\\nThere are {} notes and {} vocabs'.format(len(notes), len(set(notes))))\n",
    "    print('data loaded properly and saved to disk as notes.pkl.')\n",
    "    return notes, len(set(notes))\n",
    "#--------------------------------------------------------\n",
    "if debug : new_notes, new_n_vocab = get_notes(quick=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(notes, n_vocab):\n",
    "    ''' create input sequences and output notes '''\n",
    "    sequence_length = 100\n",
    "    NetworkInput = []\n",
    "    NetworkOutput = []\n",
    "    # create a mapping to the notes\n",
    "    mapper = LabelEncoder()\n",
    "    mapped_notes = mapper.fit_transform(notes)\n",
    "\n",
    "    for i in range(len(notes)-sequence_length):\n",
    "        in_seq = mapped_notes[i : i+sequence_length]\n",
    "        out_note = mapped_notes[i+sequence_length]\n",
    "        NetworkInput.append(in_seq)\n",
    "        NetworkOutput.append(out_note)\n",
    "    \n",
    "    n_patterns = len(NetworkOutput)\n",
    "    \n",
    "    NetworkInput = np.reshape(NetworkInput, (n_patterns, sequence_length, 1))\n",
    "    NetworkInput = NetworkInput / float(n_vocab)\n",
    "    \n",
    "    NetworkOutput = np.reshape(NetworkOutput, (-1,1))\n",
    "    hotencoder = OneHotEncoder(sparse=False)\n",
    "    _ = hotencoder.fit(mapped_notes.reshape(-1,1))\n",
    "    NetworkOutput = hotencoder.transform(NetworkOutput)\n",
    "    \n",
    "    # save the mapper and hotencoder to disk for prediction.\n",
    "    #with open('./data/mapper.pkl','wb') as f:\n",
    "    #    pickle.dump(mapper, f)\n",
    "    #with open('./data/hotencoder.pkl','wb') as f:\n",
    "    #    pickle.dump(hotencoder, f)\n",
    "    \n",
    "    print('Input shape = ',NetworkInput.shape, '\\nOutput shape = ', NetworkOutput.shape)\n",
    "    return mapped_notes, NetworkInput, NetworkOutput\n",
    "#---------------------------------------\n",
    "if debug : mapped_notes, NetworkInput, NetworkOutput = prepare_data(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, n_vocab):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = LSTM(128, activation='tanh', return_sequences=True)(X_input)\n",
    "    X = Dropout(.2)(X)\n",
    "    #X = LSTM(512, activation='tanh', return_sequences=True)(X)\n",
    "    #X = Dropout(.3)(X)\n",
    "    X = LSTM(128, activation='tanh', return_sequences=False)(X)\n",
    "    X = Dropout(.2)(X)\n",
    "    #X = Dense(256, activation='tanh')(X)\n",
    "    #X = Dropout(.3)(X)\n",
    "    X = Dense(n_vocab, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(X_input, X)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "#----------------------------------------------\n",
    "if debug : model = get_model(NetworkInput.shape[1:], n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NetworkInput, NetworkOutput):\n",
    "    filepath = './checkpoints/2/ckpt-{loss:.4f}-{epoch:02d}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks=[checkpoint]\n",
    "    \n",
    "    model.fit(NetworkInput, NetworkOutput, epochs=300, batch_size=24, callbacks=callbacks)\n",
    "    return model\n",
    "#-----------------------------------\n",
    "if debug : trained_model = train(model, NetworkInput, NetworkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_stack = []\n",
    "def train(model, NetworkInput, NetworkOutput):\n",
    "    min_loss = 40\n",
    "    avg_loss_per_epoch = 0\n",
    "    global loss_stack\n",
    "    epochs = 100\n",
    "    batch_size = 500\n",
    "    steps_per_epoch = int(np.ceil(NetworkInput.shape[0] / float(batch_size)))\n",
    "    #steps_per_epoch = 5\n",
    "    file_to_save = './checkpoints/2/ckpt-{loss:.4f}-{epoch:02d}'\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss_per_epoch = 0\n",
    "        print('starting epoch {:02d} with {} steps'.format(epoch, steps_per_epoch),end='')\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            print('.',end='')\n",
    "            \n",
    "            batch_start = step * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            if batch_end >= NetworkInput.shape[0] :\n",
    "                batch_end = NetworkInput.shape[0] - 1\n",
    "            \n",
    "            history = model.fit(NetworkInput[batch_start:batch_end], NetworkOutput[batch_start:batch_end],\n",
    "                         epochs=1, batch_size=32, verbose=0)\n",
    "            \n",
    "            current_loss = history.history['loss'][0]\n",
    "            loss_stack.append(current_loss)\n",
    "            avg_loss_per_epoch += current_loss\n",
    "            if current_loss < min_loss :\n",
    "                min_loss = current_loss\n",
    "                print('new best loss {:.4f}'.format(min_loss),end=' ')\n",
    "                model.save(file_to_save.format(loss=min_loss, epoch=epoch))\n",
    "        \n",
    "        avg_loss_per_epoch /= steps_per_epoch\n",
    "        print(\"\\nAverage loss is {:.4f}\".format(avg_loss_per_epoch))\n",
    "#----------------------------------------------------------------------\n",
    "if debug : train(model, NetworkInput, NetworkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./checkpoints/1/loss_stack.pkl','wb') as f:\n",
    "#    pickle.dump(loss_stack, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(loss_stack)),loss_stack)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
